\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{geometry}

% Algorithm packages
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}

% Additional customization for algorithms
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\Input{\item[\algorithmicinput]}
\algnewcommand\Output{\item[\algorithmicoutput]}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% Page setup
\geometry{margin=1in}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

% Custom commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}

\title{Implementation of the Lagrangian Relaxation Algorithm for Network Revenue Management}
\author{Hongzhang ``Steve'' Shao}
\date{May 18, 2025}

\begin{document}

\maketitle

\vspace{0.5cm}



% ------------------------------------------------------------------------------------------------
% ABSTRACT
% - Who is the customer? What is their need? Why do they need this solution?
% - What does this product do? How does it benefit the customer?
% - How is success measured for this product? When will it be considered successful?
% ------------------------------------------------------------------------------------------------

% \noindent
% This repository provides a public implementation of the Lagrangian Relaxation Algorithm for network revenue management, originally developed by Prof. Huseyin Topaloglu (2009). 
% Researchers in operations research often use this algorithm and its test dataset as benchmarks, but no open implementation has been available until now. 
% This code allows researchers to test, compare, and build upon the algorithm easily. 
% I have tested our implementation on all instances in Prof. Topaloglu's dataset, and the results match those reported in his paper. \\

% \noindent
% I developed this project as part of my research with Prof. Baris Ata on solving NRM problems with deep learning-based numerical methods. 
% I hope this implementation will help others support reproducible research and further work in network revenue management.

% \vspace{0.5cm}



% ------------------------------------------------------------------------------------------------
% INTRODUCTION
% ------------------------------------------------------------------------------------------------

% \section{Introduction}

% \vspace{0.5cm}



% ------------------------------------------------------------------------------------------------
% LITERATURE REVIEW
% ------------------------------------------------------------------------------------------------

\section{Resources}

This project is based on \cite{topaloglu2009using}.
Both the paper and its dataset are publicly available on Prof. Huseyin Topaloglu's \underline{\href{https://people.orie.cornell.edu/huseyin}{website}}. 
You can access the paper directly \underline{\href{https://people.orie.cornell.edu/huseyin/publications/revenue_man.pdf}{here}}.
The dataset can be downloaded from \underline{\href{https://people.orie.cornell.edu/huseyin/research/rm_datasets/rm_datasets.html}{this page}}.

\vspace{0.5cm}



% ------------------------------------------------------------------------------------------------
% SECTION
% ------------------------------------------------------------------------------------------------

\section{The NRM Problem}

Consider a \textbf{network revenue management problem} with:
\begin{itemize}[itemsep=0pt,parsep=0pt]
\item[-] A set of resources (flight legs) $\mathcal{L}$, each with capacity $c_i$ for $i\in \mathcal{L}$. 
\item[-] A set of products (itineraries) $\mathcal{J}$, each with revenue $f_j$ for $j\in \mathcal{J}$. 
    \begin{itemize}[itemsep=0pt,parsep=0pt]
    \item Each purchase of product $j$ consumes $a_{ij}$ units of capacity from resource $i$ for each $i$. 
    \end{itemize}
\item[-] Discrete time horizon $\mathcal{T}=\{1,\ldots,\tau\}$. 
\end{itemize}

\noindent
In each period $t$:
\begin{itemize}[itemsep=0pt,parsep=0pt]
\item[-] \textbf{At most one customer arrives}
\item[-] The customer requests product $j$ with probability $p_{jt}$
\item[-] $\sum_{j\in \mathcal{J}} p_{jt} \le 1$
\end{itemize}

\noindent
Note that, by adding a dummy itinerary $\psi$ with 
\begin{align*}
    f_{\psi} &= 0 \\
    a_{i\psi} &= 0 & \forall i \in \mathcal{L} \\
    p_{\psi t} &= 1 - \sum_{j\in \mathcal{J}} p_{jt} & \forall t \in \mathcal{T}
\end{align*}
It can be assumed that in each period $t$:
\begin{itemize}[itemsep=0pt,parsep=0pt]
\item[-] One customer arrives
\item[-] The customer requests product $j$ with probability $p_{jt}$
\item[-] $\sum_{j\in \mathcal{J}} p_{jt} = 1$
\end{itemize}

\noindent
Let $x_{it}$ be the remaining capacity of resource $i$ at the start of period $t$. 
Let $x_t = \bigl(x_{1t}, x_{2t}, \dots, x_{|\mathcal L|,\,t}\bigr)$ be the state vector. 
Let
\begin{align*}
    C &= \max_{\,i\in\mathcal L}\;c_i \\
    \mathcal{C} &= \{0, 1, \ldots, C\} 
\end{align*}
and let $\mathcal{C}^{|\mathcal{L}|}$ be the state space.

\vspace{0.5cm}



% ------------------------------------------------------------------------------------------------
% SECTION
% ------------------------------------------------------------------------------------------------

\section{The LR Algorithm}

\noindent
\textbf{Dynamic Programming Formulation}:
\begin{itemize}[itemsep=0pt,parsep=0pt]
\item[-] Let $u_{jt}\in\{0,1\}$ indicate whether to accept (1) or reject (0) a request for product $j$.
\item[-] Let $V_t(x_t)$ be the maximum expected revenue from period $t$ to $\tau$ given capacities $x_t$:
    \begin{align*}
        V_t(x_t) = \max_{u_t \in \mathcal{U}(x_t)} 
            \left\{ \sum_{j\in \mathcal{J}} p_{jt} 
            \left\{ 
                f_j u_{jt} + 
                V_{t+1} \left(x_t - u_{jt}\sum_{i\in \mathcal{L}}a_{ij}e_i\right) 
            \right\} \right\} 
        \tag{DP1}
    \end{align*}
    where
    \begin{align*}
        \mathcal{U}(x_t) = \left\{ 
            u_{t} \in \{0,1\}^{|\mathcal{J}|} : 
            a_{ij} u_{jt} \le x_{it} \ \ 
            \forall i \in \mathcal{L}, \ j \in \mathcal{J}
        \right\} 
    \end{align*}
    and $e_i$ is the unit vector with a 1 in the $i$-th position and 0 elsewhere.
\end{itemize}

\vspace{0.5cm}

\noindent
\textbf{Equivalent Dynamic Program}:
\begin{itemize}[itemsep=0pt,parsep=0pt]
\item[-] Let $y_{ijt}\in\{0,1\}$ indicate whether to accept (1) or reject (0) \textbf{resource} $i$ when a \textbf{request for product} $j$ arrives (e.g., it is allowed to partially accept some flight legs when an itinerary uses multiple legs).
\item[-] Let $\phi$ be a \textbf{fictitious resource} with infinite capacity. 
\item[-] Let $y_t = \{y_{ijt} : i \in \mathcal{L} \cup \{\phi\}, \ j \in \mathcal{J}\}$.
\item[-] Then, $V_t(x_t)$ can be computed as:
    \begin{align*}
        V_t(x_t) &= \max_{y_t \in \mathcal{Y}(x_t)}
            \left\{ \sum_{j\in \mathcal{J}} p_{jt} 
            \left\{ 
                f_j y_{\phi jt} + 
                V_{t+1} \left(x_t - \sum_{i\in \mathcal{L}}y_{ijt}a_{ij}e_i\right) 
            \right\} \right\}
        \tag{DP2} \\
        & \text{subject to} \quad y_{ijt} = y_{\phi jt} \quad \forall i \in \mathcal{L}, \ j \in \mathcal{J}
    \end{align*}
    where
    \begin{align*}
        \mathcal{Y}_{it}(x_t) &= \left\{ 
            y_{it} \in \{0,1\}^{|\mathcal{J}|} : 
            a_{ij} y_{ijt} \leq x_{it} \ \ 
            \forall j \in \mathcal{J} 
        \right\} \quad i \in \mathcal{L} \\
        \mathcal{Y}_{\phi t}(x_t) &= \left\{ 
            y_{\phi t} \in \{0,1\}^{|\mathcal{J}|} 
        \right\} \\
        \mathcal{Y}(x_t) &= \mathcal{Y}_{\phi t}(x_t) \prod_{i \in \mathcal{L}} \mathcal{Y}_{it}(x_t) \quad \text{(Cartesian product)}
    \end{align*}
\end{itemize}

\vspace{0.5cm}

\noindent
\textbf{Lagrangian Relaxation}:
\begin{itemize}[itemsep=0pt,parsep=0pt]
\item[-] Let $\lambda = \{\lambda_{ijt} : i \in \mathcal{L}, \ j \in \mathcal{J}, \ t \in \mathcal{T}\}$ denote the Lagrangian multiplier. 
\item[-] The Lagrangian relaxation $V^{\lambda}_t(x_t)$ is defined as:
    {\small
    \begin{align*}
        V^{\lambda}_t(x_t) &= \max_{y_t \in \mathcal{Y}(x_t)} 
            \left\{ \sum_{j\in \mathcal{J}} p_{jt} 
            \left[ 
                f_j y_{\phi jt} +
                \sum_{i \in \mathcal{L}} \lambda_{ijt} (y_{ijt} - y_{\phi jt}) + 
                V^{\lambda}_{t+1} \left(x_t - \sum_{i\in \mathcal{L}}y_{ijt}a_{ij}e_i\right) 
            \right] \right\}
            \tag{LR} 
    \end{align*}
    }%
\end{itemize}

\vspace{0.5cm}

\noindent
\textbf{Lagrangian Relaxation Algorithm}:
\begin{itemize}
\item[-] \textbf{Goal}: 
    The Lagrangian relaxation algorithm aims to find an optimal multiplier $\lambda^{*}$ that solves
    \begin{align*}
        \min_{\lambda} V^{\lambda}_{1}(c_{1}) 
        \end{align*}
    As shown in \cite{topaloglu2009using},
    \begin{align*}
        V_t(x_t) \leq V^{\lambda}_t(x_t) \quad \forall x_t \in \mathcal{C}^{|\mathcal{L}|}, \ t \in \mathcal{T}
    \end{align*}
    Therefore, $V^{\lambda^{*}}_{1}(c_{1})$ provides a tight bound to $V_{1}(c_{1})$.
\item[-] \textbf{Solving $V^{\lambda}_{1}(c_{1})$ for a given $\lambda$}: 
    It has been shown in \cite{topaloglu2009using} that $V^{\lambda}_{1}(c_{1})$ can be solved by concentrating on one resource at a time. 
    Specifically, if \(\{\vartheta^\lambda_{it}(x_{it}): x_{it} \in \mathcal{C}, t \in \mathcal{T}\}\) is a solution to the optimality equation
    \begin{align*}
        \vartheta^\lambda_{it}(x_{it}) 
        = \max_{y_{it} \in \mathcal{Y}_{it}(x_{it})} 
        \left\{ 
            \sum_{j \in \mathcal{J}} p_{jt} 
            \left[ 
                \lambda_{ijt} y_{ijt} 
                + \vartheta^\lambda_{i, t+1}(x_{it} - a_{ij} y_{ijt}) 
            \right] 
        \right\}
        \tag{SDP}
    \end{align*}
    for all \(i \in \mathcal{L}\), then
    \begin{align}
        V^\lambda_t(x_t) 
        = \sum_{t' = t}^\tau \sum_{j \in \mathcal{J}} p_{jt'} 
            \left[ f_j - \sum_{i \in \mathcal{L}} \lambda_{ijt'} \right]^+ 
        + \sum_{i \in \mathcal{L}} \vartheta^\lambda_{it}(x_{it}),
        \label{eq:lagrangian_relaxation_algorithm}
    \end{align}
    where $[z]^+ = \max\{z, 0\}$.
\item[-] \textbf{Minimizing $V^{\lambda}_{1}(c_{1})$ over $\lambda$}: 
    It has also been shown in \cite{topaloglu2009using} that the Lagrangian relaxation $V^{\lambda}_{1}(c_{1})$ is convex in $\lambda$. 
    Thus, the optimal multiplier $\lambda^*$ can be found by using classical subgradient methods.
\end{itemize}

\vspace{0.5cm}

\noindent
\textbf{Control Policy}:
\begin{itemize}
\item[-] The control policy is to accept a request for product $j$ at time $t$ if and only if:
    \begin{align*}
        f_j \geq \sum_{i \in \mathcal{L}} \sum_{r = 1}^{a_{ij}} \left[ \vartheta^{\lambda^{*}}_{i,t+1}(x_{it} - r + 1) - \vartheta^{\lambda^{*}}_{i,t+1}(x_{it} - r) \right]
    \end{align*}
    That is, a product is accepted if its revenue exceeds the opportunity cost of consumed resources. 
    Specifically, the term $\vartheta^{\lambda^{*}}_{i,t+1}(x_{it}) - \vartheta^{\lambda^{*}}_{i,t+1}(x_{it} - 1)$ represents the bid price of resource $i$ at time $t$.
\end{itemize}


\vspace{0.5cm}



% ------------------------------------------------------------------------------------------------
% SECTION
% ------------------------------------------------------------------------------------------------

\section{Implementation}

\noindent
\textbf{Note:} \cite{topaloglu2009using} did not provide any specific implementation details or the choice of algorithms. Thus, I need to do my own implementation. What follows are my own implementation steps.

\vspace{0.5cm}



% ------------------------------------------------------------------------------------------------

\subsection{Subroutine: Solving the Single-Resource Dynamic Program}

\noindent
We solve the single-resource optimality equation using tabular backward induction:

\begin{algorithm}[H]
\caption{Subroutine: Tabular Backward Induction for Single-Resource Dynamic Program}
\begin{algorithmic}[1]
\Require Resource $i$, capacities $\mathcal{C}$, time periods $\mathcal{T}$, probabilities $p_{jt}$, consumption $a_{ij}$, multipliers $\lambda_{ijt}$
\Ensure Value functions $\vartheta^\lambda_{it}(x_{it})$ and optimal decisions $y^*_{ijt}(x_{it})$
\State Initialize $\vartheta^\lambda_{i,\tau+1}(x_{i,\tau+1}) \gets 0$ for all $x_{i,\tau+1} \in \mathcal{C}$ \Comment{Terminal condition}
\For{$t = \tau$ \textbf{down to} $1$} \Comment{Backward recursion}
    \For{$x_{it} = 0$ \textbf{to} $C$} \Comment{For each capacity level}
        \State $\vartheta^\lambda_{it}(x_{it}) \gets 0$
        \State $y^*_{ijt}(x_{it}) \gets 0$ for all $j \in \mathcal{J}$ \Comment{Initialize decision variables}
        \For{$j \in \mathcal{J}$} \Comment{For each product}
            \If{$a_{ij} \leq x_{it}$} \Comment{Check if capacity is sufficient}
                \State $v_0 \gets \vartheta^\lambda_{i,t+1}(x_{it})$ \Comment{Value if reject}
                \State $v_1 \gets \lambda_{ijt} + \vartheta^\lambda_{i,t+1}(x_{it} - a_{ij})$ \Comment{Value if accept}
                \If{$v_1 > v_0$}
                    \State $y^*_{ijt}(x_{it}) \gets 1$ \Comment{Accept product $j$ at time $t$ with capacity $x_{it}$}
                \EndIf
            \EndIf
        \EndFor
        \State $\displaystyle \vartheta^\lambda_{it}(x_{it}) \gets \sum_{j\in\mathcal{J}} p_{jt}\Bigl[ \lambda_{ijt}\,y^*_{ijt}(x_{it}) + \vartheta^\lambda_{i,t+1}\!\bigl(x_{it}-a_{ij}y^*_{ijt}(x_{it})\bigr) \Bigr]$
    \EndFor
\EndFor
\State \Return $\{\vartheta^\lambda_{it}(x_{it}): x_{it} \in \mathcal{C}, t \in \mathcal{T}\}$ and $\{y^*_{ijt}(x_{it}): j \in \mathcal{J}, x_{it} \in \mathcal{C}, t \in \mathcal{T}\}$
\end{algorithmic}
\end{algorithm}

\vspace{0.5cm}

{\color{blue}


% ------------------------------------------------------------------------------------------------

\subsection{Subroutine: Computing the Subgradient of $\vartheta^\lambda_{it}(x_{it})$}
\label{subroutine:computing_subgradient}

\noindent
Consider any resource $i$. 
Note that: 
\begin{align*}
    \vartheta^\lambda_{it}(x_{it}) 
    &= \max_{y_{it} \in \mathcal{Y}_{it}(x_{it})} 
    \left\{ 
        \sum_{j \in \mathcal{J}} p_{jt} 
        \left[ 
            \lambda_{ijt} y_{ijt} 
            + \vartheta^\lambda_{i, t+1}(x_{it} - a_{ij} y_{ijt}) 
        \right] 
    \right\} \\
    &= \sum_{j \in \mathcal{J}} p_{jt} 
        \left[ 
            \lambda_{ijt} y^{*\lambda}_{ijt} 
            + \vartheta^\lambda_{i, t+1}(x_{it} - a_{ij} y^{*\lambda}_{ijt}) 
        \right] . 
\end{align*}
Let \( \mu_{it}(x_{it}) \) be the probability mass function of the capacity of resource $i$ at time $t$. 
It is given by:
\begin{align*}
    \mu_{i,t+1}(x') &= \sum_{x} \mu_{it}(x) \sum_{j \in \mathcal{J}} p_{jt} \mathbb{I}\left\{x' = x - a_{ij}y_{ijt}^*(x)\right\} .
\end{align*}
To compute the subgradient of $\vartheta^\lambda_{i1}(c_{i})$ with respect to $\lambda_{ijt}$, let
\begin{align*}
    \mu_{i1}(x) &= 
        \begin{cases}
        1 & \text{if } x = c_i, \\
        0 & \text{otherwise.}
        \end{cases}
\end{align*}
Then, we have:
\begin{align*}
    \frac{\partial \vartheta^\lambda_{i1}(c_{i})}{\partial \lambda_{ijt}} 
    &= \sum_{x} \mu_{it}(x) y^{*\lambda}_{ijt}(x) 
\end{align*}

\begin{algorithm}[H]
\caption{Forward Pass for Computing $\mu_{it}(x_{it})$}
\begin{algorithmic}[1]
    \State \textbf{Initialize:} $\mu_{i1}(x) \gets 1$ if $x = c_i$, else $0$ \Comment{$x \in \mathcal{C} = \{0, 1, \ldots, C\}$}
    \For{$t = 1$ to $\tau-1$}
        \For{each $x'$ in $\mathcal{C}$} \Comment{$x' \in \mathcal{C} = \{0, 1, \ldots, C\}$}
            \State $\mu_{i, t+1}(x') \gets 0$
        \EndFor
        \For{each $x$ in $\mathcal{C}$} \Comment{$x \in \mathcal{C} = \{0, 1, \ldots, C\}$}
            \For{each $j \in \mathcal{J}$}
                \State $x' \gets x - a_{ij} y_{ijt}^*(x)$
                \If{$x'$ is a valid capacity} \Comment{$x' \in \mathcal{C}$}
                    \State $\mu_{i, t+1}(x') \gets \mu_{i, t+1}(x') + \mu_{it}(x) \cdot p_{jt}$
                \EndIf
            \EndFor
        \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}

\vspace{0.5cm}



% ------------------------------------------------------------------------------------------------

\subsection{Subgradient Optimization: Alternative 1}

% \noindent
% After solving the single-resource dynamic programs, we obtain the optimal decisions $y^{*\lambda}_{ijt}(x_{it})$ for all physical resources $i \in \mathcal{L}$. For the fictitious resource $\phi$, the optimal decision is:

% $$y^{*\lambda}_{\phi jt} = 
% \begin{cases} 
% 1 & \text{if } f_j - \sum_{i \in \mathcal{L}} \lambda_{ijt} \ge 0 \\
% 0 & \text{otherwise}
% \end{cases}$$

% \noindent
% The complete optimal solution to the Lagrangian Relaxation is therefore:
% \begin{itemize}[itemsep=0pt,parsep=0pt]
% \item[-] $y^{*\lambda}_{ijt}(x_{it})$ for all $i \in \mathcal{L}$, $j \in \mathcal{J}$, $t \in \mathcal{T}$, $x_{it} \in \mathcal{C}$ (from backward induction)
% \item[-] $y^{*\lambda}_{\phi jt}$ for all $j \in \mathcal{J}$, $t \in \mathcal{T}$ (as defined above)
% \end{itemize}

\noindent
Using \eqref{eq:lagrangian_relaxation_algorithm} and Subsection \ref{subroutine:computing_subgradient}, we have: 
\begin{align*}
    \frac{\partial V^{\lambda}_{1}(c_{1})}{\partial \lambda_{ijt}} 
    &= \frac{\partial \vartheta^\lambda_{i1}(c_{i})}{\partial \lambda_{ijt}} 
    - p_{jt} \,\mathbf{1}\left\{f_j - \sum_{k \in \mathcal{L}} \lambda_{kjt} \ge 0\right\} 
\end{align*}
Note that as a Lagrangian multiplier, $\lambda_{ijt} \ge 0$. 
Thus, here we use a projected subgradient descent algorithm to optimize $\lambda$.
\begin{algorithm}[H]
\caption{Projected Subgradient Descent for Lagrangian Multiplier Optimization}
\begin{algorithmic}[1]
\Require Initial multipliers $\lambda^0$, step size $\alpha_0$, tolerance $\epsilon$, maximum iterations $K$
\Ensure Optimized multipliers $\lambda^*$
\State $k \gets 0$
\State $V_{\text{prev}} \gets \infty$
\While{$k < K$}
    \State Compute $V^{\lambda^k}_{1}(c_{1})$
    \If{$|V^{\lambda^k}_{1}(c_{1}) - V_{\text{prev}}| < \epsilon$}
        \State \textbf{break}
    \EndIf
    \State $V_{\text{prev}} \gets V^{\lambda^k}_{1}(c_{1})$
    \State Compute subgradient $g^k$ of $V^{\lambda^k}_{1}(c_{1})$ with respect to $\lambda$. 
    \State $\alpha_k \gets \frac{\alpha_0}{\sqrt{k+1}}$
    \State $\lambda^{k+1} \gets \max\{0, \lambda^k - \alpha_k g^k\}$ \Comment{Project onto feasible set}
    \State $k \gets k + 1$
\EndWhile
\State \Return $\lambda^k$
\end{algorithmic}
\end{algorithm}


}%

\vspace{0.5cm}



% ------------------------------------------------------------------------------------------------

\subsection{Subgradient Optimization: Alternative 2}

\noindent
Without computing subgradient analytically, we can use the following algorithm to optimize $\lambda$: 

\begin{algorithm}[H]
\caption{Subgradient Optimization for Lagrangian Relaxation}
\begin{algorithmic}[1]
\Require Initial multiplier $\lambda^0$, initial step size $\alpha_0$, tolerance $\epsilon$, max iterations $K$
\Ensure Optimal multiplier $\lambda^*$
\State $k \gets 0$
\State $V_{\text{prev}} \gets \infty$
\While{$k < K$}
    \State Compute $V^{\lambda^k}_{1}(c_{1})$ \Comment{Evaluate current objective}
    \If{$|V^{\lambda^k}_{1}(c_{1}) - V_{\text{prev}}| < \epsilon$}
        \State \textbf{break} \Comment{Convergence achieved}
    \EndIf
    \State $V_{\text{prev}} \gets V^{\lambda^k}_{1}(c_{1})$
    \State Generate random unit vector $d$
    \State Compute $V^{\lambda^k + \delta d}_{1}(c_{1})$ \Comment{Perturbed objective}
    \State $g^k \gets \frac{V^{\lambda^k + \delta d}_{1}(c_{1}) - V^{\lambda^k}_{1}(c_{1})}{\delta} \cdot d$ \Comment{Approximate subgradient}
    \State $\alpha_k \gets \frac{\alpha_0}{\sqrt{k+1}}$ \Comment{Update step size}
    \State $\lambda^{k+1} \gets \max\{0, \lambda^k - \alpha_k g^k\}$ \Comment{Component-wise projection onto $\Lambda = \{\lambda \geq 0\}$}
    \State $k \gets k + 1$
\EndWhile
\State \Return $\lambda^k$
\end{algorithmic}
\end{algorithm}

\vspace{0.5cm}



% ------------------------------------------------------------------------------------------------
% SECTION
% ------------------------------------------------------------------------------------------------

\bibliographystyle{apalike}
\bibliography{references}

\end{document}
